{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10863022,"sourceType":"datasetVersion","datasetId":6748424},{"sourceId":10873475,"sourceType":"datasetVersion","datasetId":6755873},{"sourceId":10877017,"sourceType":"datasetVersion","datasetId":6758197},{"sourceId":11396784,"sourceType":"datasetVersion","datasetId":7137654}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# dependecy","metadata":{}},{"cell_type":"code","source":"pip install sentence-transformers langchain_community unstructured faiss-cpu Together together\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T05:13:38.993363Z","iopub.execute_input":"2025-08-12T05:13:38.993698Z","iopub.status.idle":"2025-08-12T05:14:02.566704Z","shell.execute_reply.started":"2025-08-12T05:13:38.993673Z","shell.execute_reply":"2025-08-12T05:14:02.565572Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Code","metadata":{}},{"cell_type":"code","source":"import json\n\n\nwith open(\"/kaggle/input/final-database/legal_database.json\", \"r\", encoding=\"utf-8\") as f:\n    qa_data = json.load(f)\n\n\nquestions = [item[\"question\"] for item in qa_data]\nanswers = {item[\"question\"]: item[\"answer\"] for item in qa_data}  # Mapping Q -> A\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T05:14:02.568278Z","iopub.execute_input":"2025-08-12T05:14:02.568644Z","iopub.status.idle":"2025-08-12T05:14:02.676661Z","shell.execute_reply.started":"2025-08-12T05:14:02.568617Z","shell.execute_reply":"2025-08-12T05:14:02.675956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\n\nmodel = SentenceTransformer(\"sentence-transformers/msmarco-distilbert-base-v4\")  \nmodel.save(\"qa_model\")\n\nquestion_embeddings = model.encode(questions, convert_to_tensor=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T05:14:02.678307Z","iopub.execute_input":"2025-08-12T05:14:02.678601Z","iopub.status.idle":"2025-08-12T05:14:22.224233Z","shell.execute_reply.started":"2025-08-12T05:14:02.678572Z","shell.execute_reply":"2025-08-12T05:14:22.223329Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n\ntorch.save({\n    \"embeddings\": question_embeddings,\n    \"questions\": questions\n}, \"question_embeddings.pt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T05:14:22.225699Z","iopub.execute_input":"2025-08-12T05:14:22.226288Z","iopub.status.idle":"2025-08-12T05:14:22.354358Z","shell.execute_reply.started":"2025-08-12T05:14:22.226233Z","shell.execute_reply":"2025-08-12T05:14:22.353232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import DirectoryLoader, PyPDFDirectoryLoader\n\n\ntxt_loader = DirectoryLoader('/kaggle/input/indian-law', glob=\"*.txt\")\npdf_loader = PyPDFDirectoryLoader('/kaggle/input/indian-law')  \n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=100)\n\n\ntxt_documents = txt_loader.load_and_split(text_splitter)\npdf_documents = pdf_loader.load_and_split(text_splitter)\n\n\nall_documents = txt_documents + pdf_documents\nchunks = [doc.page_content for doc in all_documents]\n\n\nnp.save(\"legal_chunks.npy\", np.array(chunks, dtype=object))\n\nprint(f\"✅ Processed {len(chunks)} legal text chunks from TXT & PDF.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T05:14:22.355447Z","iopub.execute_input":"2025-08-12T05:14:22.355722Z","iopub.status.idle":"2025-08-12T05:15:54.819042Z","shell.execute_reply.started":"2025-08-12T05:14:22.355698Z","shell.execute_reply":"2025-08-12T05:15:54.818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"chunks[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T05:15:54.820099Z","iopub.execute_input":"2025-08-12T05:15:54.820942Z","iopub.status.idle":"2025-08-12T05:15:54.826613Z","shell.execute_reply.started":"2025-08-12T05:15:54.820908Z","shell.execute_reply":"2025-08-12T05:15:54.825868Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain_community.embeddings import HuggingFaceEmbeddings\n\n\n\nembedding_model = HuggingFaceEmbeddings(model_name=\"law-ai/InLegalBERT\")\n\n\nlegal_embeddings = np.array([embedding_model.embed_query(chunk) for chunk in chunks])\n\n\nnp.save(\"legal_embeddings.npy\", legal_embeddings)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T05:15:54.827656Z","iopub.execute_input":"2025-08-12T05:15:54.82799Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_path = \"legal_model\"\nembedding_model.client.save_pretrained(model_path) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import faiss\n\nlegal_embeddings = np.load(\"/kaggle/working/legal_embeddings.npy\", allow_pickle=True)\ndim = legal_embeddings.shape[1]\n\n\nfaiss_index = faiss.IndexFlatL2(dim)\n\n\nfaiss_index.add(legal_embeddings)\n\n\nfaiss.write_index(faiss_index, \"legal_faiss.index\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import together\ntogether.api_key = \"tgp_v1_hcAYeb5IquESKVQOsx6_wbAn0jkRNJTWHnNipFUTIlI\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport torch\nimport faiss\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\n\nmodel = SentenceTransformer(\"/kaggle/working/qa_model\")\nembedding_model = HuggingFaceEmbeddings(model_name=\"/kaggle/working/legal_model\")\n\n\ndata = torch.load(\"question_embeddings.pt\",weights_only=False)\nquestion_embeddings = data[\"embeddings\"]\nquestions = data[\"questions\"]\nfaiss_index = faiss.read_index(\"/kaggle/working/legal_faiss.index\")\nlegal_chunks = np.load(\"/kaggle/working/legal_chunks.npy\", allow_pickle=True)\n\n\n\ndef find_closest_match(user_question):\n    \n    user_embedding = model.encode(user_question, convert_to_tensor=True)\n    \n    similarities = util.pytorch_cos_sim(user_embedding, question_embeddings)\n    best_match_idx = similarities.argmax().item()\n    \n    best_match_question = questions[best_match_idx]\n    similarity_score = similarities[0][best_match_idx].item()\n\n    return best_match_question, similarity_score\n\n\ndef search_legal_docs(query,top_k=5):\n    \n    query_embedding = np.array(embedding_model.embed_query(query)).reshape(1, -1)\n    distances, indices = faiss_index.search(query_embedding, top_k)\n    \n    retrieved_chunks = [legal_chunks[idx] for idx in indices[0]]\n\n    \n    query_vector = torch.tensor(query_embedding, dtype=torch.float32)\n    chunk_vectors = torch.tensor([embedding_model.embed_query(chunk) for chunk in retrieved_chunks], dtype=torch.float32)\n\n    scores = util.pytorch_cos_sim(query_vector, chunk_vectors)[0]\n    ranked_chunks = sorted(zip(retrieved_chunks, scores), key=lambda x: x[1], reverse=True)\n    best_chunk, _ = ranked_chunks[0]\n    model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"  # Choose another if needed\n\n    prompt = f\"\"\"\n    As a legal chatbot specializing in the Indian Penal Code, you are tasked with providing highly accurate and contextually appropriate responses. Ensure your answers meet these criteria:\n- First of all state the law in which the context comes in. Then Respond in a 5 bullet-point format to clearly delineate distinct aspects of the legal query.\n- Each point should accurately reflect the legal provision in question, avoiding over-specificity unless directly relevant to the user's query.\n- Clarify the general applicability of the legal rules or sections mentioned, highlighting any common misconceptions or frequently misunderstood aspects.\n- Limit responses to essential information that directly addresses the user's question, providing concise yet comprehensive explanations.\n- Avoid assuming specific contexts or details not provided in the query, focusing on delivering universally applicable legal interpretations unless otherwise specified.\n\n\nCONTEXT: {best_chunk}\n\nQUESTION: {query}\nANSWER:\n\n\n    \"\"\"\n    \n    response = together.Complete.create(\n        model=model_name,\n        prompt=prompt,\n        max_tokens=400,\n        temperature=1\n    )\n    \n\n    if \"choices\" in response and response[\"choices\"]:\n        \n        return response[\"choices\"][0][\"text\"].strip()\n    else:\n        return \"Error: No response received from Together AI.\"\n\ndef get_answer(user_question):\n    \n    best_match_question, similarity_score = find_closest_match(user_question)\n\n    if similarity_score >= 0.7:\n        return f\"✅ Exact Match Found({similarity_score:.2f}):\\n{answers[best_match_question]}\"\n\n    elif 0.3 < similarity_score < 0.7:\n        top_chunks = search_legal_docs(user_question)\n        return f\"⚖️ Legal Text Found:\\n{top_chunks}\"  \n\n    else:\n        return \"Sorry cant give response at the moment.\"  \n\n\nuser_question = \"i got in an accident. What should i do?\"\nresponse = get_answer(user_question)\nprint(response)\n","metadata":{"trusted":true,"_kg_hide-input":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## location awareness","metadata":{}},{"cell_type":"code","source":"import requests\nimport json\nimport re\n\napi_key = \"AlzaSymw9AJ0q15jTp12FlRKCqCHrNHH54hWoxJ\"\nquery = \"law firms in dharwad\"\n\n\nurl = f\"https://maps.gomaps.pro/maps/api/place/textsearch/json?query={query}&key={api_key}\"\nresponse = requests.get(url).json()\n\n\nplaces = []\ndef get_photo_href(photo_info):\n    if photo_info:\n        attributions = photo_info.get(\"html_attributions\", [])\n        if attributions:\n            links = [re.search(r'href=\"([^\"]+)\"', attr).group(1) for attr in attributions if 'href=\"' in attr]\n            return links[0] if links else \"No attribution link\"\n    return \"No attribution link\"\n\n\nfor place in response.get(\"results\", []):\n    name = place.get(\"name\", \"Unknown\")\n    address = place.get(\"formatted_address\", \"No address found\")\n    rating = place.get(\"rating\", \"No rating\")\n\n    photo_info = place.get(\"photos\", [{}])[0]\n    reference = get_photo_href(photo_info)\n    \n    places.append(f\"{name}, Address: {address}, Rating: {rating}, reference:{reference}\")\n\n\nplaces_text = \"\\n\\n\".join(places)\n\n\nprompt = f\"\"\"\nBased on the given legal places, provide the reference, name, address ,rating  of the most relevant ones to the user:\n\n{places_text}\n\nOnly return the 3 most relevant results in a user-friendly way.\n\"\"\"\nresponse = together.Complete.create(\n        model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n        prompt=prompt,\n        max_tokens=500,\n        temperature=0\n    )\n    \n\nif \"choices\" in response and response[\"choices\"]:\n        \n        print(response[\"choices\"][0][\"text\"].strip())\nelse:\n        print( \"Error: No response received from Together AI.\")\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## legal advice from lawyers","metadata":{}},{"cell_type":"code","source":"import json\nimport numpy as np\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\n# Load your dataset\nwith open(\"/kaggle/input/legal-advice/answers_data.json\", \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\n\n# Storage containers\nprocessed_data = []\n\n\nfor entry in data:\n    full_text = entry[\"full_text\"]\n    answers = entry[\"answers\"]\n    question_url = entry[\"question_url\"]\n\n    # Join all answers into one string\n    joined_answers = \"\\n\".join(answers)\n\n    # Store only necessary fields\n    processed_data.append({\n        \"question_url\": question_url,\n        \"full_text\": full_text,\n        \"joined_answers\": joined_answers\n    })\n\n# Save cleaned and processed data\nwith open(\"processed_lawyer_data.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(processed_data, f, indent=2, ensure_ascii=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## summarize question and answer before embedding","metadata":{}},{"cell_type":"code","source":"import json\n\n# Load the JSON data\nwith open('/kaggle/working/processed_lawyer_data.json', 'r') as file:\n    data = json.load(file)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BartTokenizer, BartForConditionalGeneration\ntokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\nmodel = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"bart_summarizer\")\ntokenizer.save_pretrained(\"bart_summarizer\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import BartTokenizer, BartForConditionalGeneration\nfrom tqdm import tqdm\nimport torch\n\n# Load your dataset\ndf = pd.read_json(\"/kaggle/working/processed_lawyer_data.json\")  # Or use pd.read_csv(...) depending on your format\n\n# Load BART model and tokenizer\nmodel = BartForConditionalGeneration.from_pretrained(\"/kaggle/working/bart_summarizer\", forced_bos_token_id=0)\ntokenizer = BartTokenizer.from_pretrained(\"/kaggle/working/bart_summarizer\")\nmodel.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Function to summarize a single text\ndef summarize(text, max_input_length=1024, max_output_length=350):\n    prompt = (\n    \"You are a legal assistant. Summarize the following legal situation by extracting:\\n\"\n    \"- The legal dispute and its current status\\n\"\n    \"- Actions taken by the people involved\\n\"\n    \"- The legal question or help being asked\\n\\n\"\n    \"Text:\\n\" + text\n    )\n    inputs = tokenizer(prompt, max_length=1024, truncation=True, return_tensors=\"pt\").to(model.device)\n\n    summary_ids = model.generate(\n        inputs[\"input_ids\"],\n        num_beams=6,\n        max_length=max_output_length,\n        min_length=120,\n        no_repeat_ngram_size=3,\n        length_penalty=1.2,\n        early_stopping=True\n    )\n\n    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n# Prepare new columns for summaries\ndf[\"fulltext_summary\"] = \"\"\ndf[\"answers_summary\"] = \"\"\n\n# Summarize in batches\nfor idx in tqdm(range(len(df))):\n    try:\n        full = df.loc[idx, \"full_text\"]\n        ans = df.loc[idx, \"joined_answers\"]\n\n        df.at[idx, \"fulltext_summary\"] = summarize(full)\n        df.at[idx, \"answers_summary\"] = summarize(ans)\n    except Exception as e:\n        print(f\"Error at index {idx}: {e}\")\n        continue\n\n# Save the summarized dataset\ndf.to_json(\"summarized_legal_data.json\", orient=\"records\", indent=2)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_json(\"/kaggle/working/summarized_legal_data.json\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_legal_advice_data=df[[\"answers_summary\", \"fulltext_summary\", \"question_url\"]]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_legal_advice_data.to_json(\"final_summarized_data.json\", orient=\"records\", indent=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## fine tuning inlegalbert","metadata":{}},{"cell_type":"code","source":"import json\n\nwith open(\"/kaggle/working/final_summarized_data.json\", \"r\") as f:\n    data = json.load(f)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, models\n\n# Load INLegalBERT model (from HuggingFace or local path)\nword_embedding_model = models.Transformer(\"law-ai/InLegalBERT\", max_seq_length=512)\n\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import losses, InputExample, SentenceTransformer\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n\ntrain_examples = [\n    InputExample(texts=[item[\"fulltext_summary\"], item[\"answers_summary\"]], label=1) for item in        data\n]\ntrain_dataloader = DataLoader(train_examples, shuffle=True, batch_size=4)\n\ntrain_loss = losses.CosineSimilarityLoss(model=model)\n\n# Step 6: Train the model\nmodel.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    epochs=3,\n    warmup_steps=100,\n    show_progress_bar=True\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"fine-tuned-inlegalbert\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## creating faiss index and cross encoder for similarity search","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"/kaggle/working/fine-tuned-inlegalbert\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import faiss\nfull_texts = [item[\"fulltext_summary\"] for item in data]\nanswers = [item[\"answers_summary\"] for item in data]  # in case you want to use later\n\n# Embed all full_texts\nfull_text_embeddings = model.encode(full_texts, convert_to_numpy=True, show_progress_bar=True)\n\n# Create FAISS index (for cosine similarity use IndexFlatIP and normalize)\ndimension = full_text_embeddings.shape[1]\nindex = faiss.IndexFlatIP(dimension)\n\n# Normalize embeddings (important for cosine similarity)\nfaiss.normalize_L2(full_text_embeddings)\n\n# Add to FAISS\nindex.add(full_text_embeddings)\n\n# Save the index and texts (optional)\nfaiss.write_index(index, \"final_faiss_fulltext.index\")\nwith open(\"full_texts.json\", \"w\") as f:\n    json.dump(full_texts, f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import CrossEncoder\n\n# Load a cross-encoder model trained for relevance ranking\nreranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## legal advice","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport faiss\nindex=faiss.read_index(\"/kaggle/working/final_faiss_fulltext.index\")\ndef get_similar_advice(user_question, top_k=10, threshold=0.2):\n    # Step 1: Encode query\n    query_embedding = model.encode(user_question, convert_to_numpy=True)\n    query_embedding = np.array(query_embedding).reshape(1, -1).astype('float32')\n    faiss.normalize_L2(query_embedding)\n\n   \n    D, I = index.search(query_embedding, top_k)\n    similarity_scores = D[0]\n\n    candidates = []\n    for idx, i in enumerate(I[0]):\n        if i < len(data):\n            item = data[i].copy()\n            item[\"similarity\"] = float(similarity_scores[idx])\n            candidates.append(item)\n\n    # Step 3: Rerank\n    rerank_pairs = [(user_question, item[\"fulltext_summary\"]) for item in candidates]\n    scores = reranker.predict(rerank_pairs)\n\n    for i, item in enumerate(candidates):\n        item[\"rerank_score\"] = scores[i]\n\n    # Step 4: Sort by rerank score\n    sorted_candidates = sorted(candidates, key=lambda x: x[\"rerank_score\"], reverse=True)\n\n    \n    final_results = []\n    for item in sorted_candidates:\n        if item[\"similarity\"] >= threshold:\n            final_results.append({\n                \"answers\": item[\"answers_summary\"],\n                \"url\": item.get(\"question_url\", None),\n                \"similarity\": round(item[\"similarity\"], 3),\n                \"rerank_score\": round(item[\"rerank_score\"], 3)\n            })\n\n    if not final_results:\n        return [{\n            \"answers\": \"No relevant legal advice found for your query.\",\n            \"url\": None,\n            \"similarity\": 0,\n            \"rerank_score\": 0\n        }]\n\n    return final_results\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"user_question = \"can i get forced to get divorce because of normal fights\"\nresponses = get_similar_advice(user_question)\n\nfor i, res in enumerate(responses, 1):\n    print(f\"\\n--- Response {i} ---\")\n    print(\"Answers:\\n\", res[\"answers\"])\n    print(\"URL:\", res[\"url\"])\n    print(\"Similarity:\", res[\"similarity\"])\n    print(\"Rerank Score:\", res[\"rerank_score\"])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}